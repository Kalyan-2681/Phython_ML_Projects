{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python392jvsc74a57bd0594466f298bc55e3ef327a3727807711fea6f7d5db7af4217c328027f6bbcf6f",
   "display_name": "Python 3.9.2 64-bit"
  },
  "metadata": {
   "interpreter": {
    "hash": "594466f298bc55e3ef327a3727807711fea6f7d5db7af4217c328027f6bbcf6f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set_style(\"darkgrid\")\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "], [1, 4], [2, 0], [2, 1], [2, 2], [2, 3], [2, 4], [3, 0], [3, 1], [3, 2], [3, 3], [3, 4], [4, 0], [4, 1], [4, 2], [4, 3], [4, 4]]\n--------------new State--------\n [0, 0]\naction [-1, 0]\nreward 0\nvalueMap[finalPosition[0], finalPosition[1] 0.0\nweightedvalue 0.0\ncopyValueMap[state[0], state[1]] 0.0\naction [1, 0]\nreward 0\nvalueMap[finalPosition[0], finalPosition[1] -1.0\nweightedvalue -0.25\ncopyValueMap[state[0], state[1]] -1.0\naction [0, 1]\nreward 0\nvalueMap[finalPosition[0], finalPosition[1] -1.25\nweightedvalue -0.5625\ncopyValueMap[state[0], state[1]] -1.25\naction [0, -1]\nreward 0\nvalueMap[finalPosition[0], finalPosition[1] -1.5625\nweightedvalue -0.953125\ncopyValueMap[state[0], state[1]] -1.5625\n--------------new State--------\n [0, 1]\naction [-1, 0]\ninitial [0, 1]\naction [-1, 0]\nreward -0.25\nvalueMap[finalPosition[0], finalPosition[1] 0.0\nweightedvalue -0.0625\ncopyValueMap[state[0], state[1]] 0.0\naction [1, 0]\ninitial [0, 1]\naction [1, 0]\nreward -0.25\nvalueMap[finalPosition[0], finalPosition[1] 0.0\nweightedvalue -0.125\ncopyValueMap[state[0], state[1]] -1.0625\naction [0, 1]\ninitial [0, 1]\naction [0, 1]\nreward -0.25\nvalueMap[finalPosition[0], finalPosition[1] 0.0\nweightedvalue -0.1875\ncopyValueMap[state[0], state[1]] -1.125\naction [0, -1]\ninitial [0, 1]\naction [0, -1]\nreward -0.25\nvalueMap[finalPosition[0], finalPosition[1] -1.953125\nweightedvalue -0.73828125\ncopyValueMap[state[0], state[1]] -1.1875\n--------------new State--------\n [0, 2]\naction [-1, 0]\ninitial [0, 2]\naction [-1, 0]\nreward -0.25\nvalueMap[finalPosition[0], finalPosition[1] 0.0\nweightedvalue -0.0625\ncopyValueMap[state[0], state[1]] 0.0\naction [1, 0]\ninitial [0, 2]\naction [1, 0]\nreward -0.25\nvalueMap[finalPosition[0], finalPosition[1] 0.0\nweightedvalue -0.125\ncopyValueMap[state[0], state[1]] -1.0625\naction [0, 1]\ninitial [0, 2]\naction [0, 1]\nreward -0.25\nvalueMap[finalPosition[0], finalPosition[1] 0.0\nweightedvalue -0.1875\ncopyValueMap[state[0], state[1]] -1.125\naction [0, -1]\ninitial [0, 2]\naction [0, -1]\nreward -0.25\nvalueMap[finalPosition[0], finalPosition[1] -1.73828125\nweightedvalue -0.6845703125\ncopyValueMap[state[0], state[1]] -1.1875\n--------------new State--------\n [0, 3]\naction [-1, 0]\ninitial [0, 3]\naction [-1, 0]\nreward -0.25\nvalueMap[finalPosition[0], finalPosition[1] 0.0\nweightedvalue -0.0625\ncopyValueMap[state[0], state[1]] 0.0\naction [1, 0]\ninitial [0, 3]\naction [1, 0]\nreward -0.25\nvalueMap[finalPosition[0], finalPosition[1] 0.0\nweightedvalue -0.125\ncopyValueMap[state[0], state[1]] -1.0625\naction [0, 1]\ninitial [0, 3]\naction [0, 1]\nreward -0.25\nvalueMap[finalPosition[0], finalPosition[1] 0.0\nweightedvalue -0.1875\ncopyValueMap[state[0], state[1]] -1.125\naction [0, -1]\ninitial [0, 3]\naction [0, -1]\nreward -0.25\nvalueMap[finalPosition[0], finalPosition[1] -1.6845703125\nweightedvalue -0.671142578125\ncopyValueMap[state[0], state[1]] -1.1875\n--------------new State--------\n [0, 4]\naction [-1, 0]\ninitial [0, 4]\naction [-1, 0]\nreward -0.25\nvalueMap[finalPosition[0], finalPosition[1] 0.0\nweightedvalue -0.0625\ncopyValueMap[state[0], state[1]] 0.0\naction [1, 0]\ninitial [0, 4]\naction [1, 0]\nreward -0.25\nvalueMap[finalPosition[0], finalPosition[1] 0.0\nweightedvalue -0.125\ncopyValueMap[state[0], state[1]] -1.0625\naction [0, 1]\ninitial [0, 4]\naction [0, 1]\nreward -0.25\nvalueMap[finalPosition[0], finalPosition[1] -1.125\nweightedvalue -0.46875\ncopyValueMap[state[0], state[1]] -1.125\naction [0, -1]\ninitial [0, 4]\naction [0, -1]\nreward -0.25\nvalueMap[finalPosition[0], finalPosition[1] -1.671142578125\nweightedvalue -0.94903564453125\ncopyValueMap[state[0], state[1]] -1.46875\n--------------new State--------\n [1, 0]\naction [-1, 0]\ninitial [1, 0]\naction [-1, 0]\nreward -0.25\nvalueMap[finalPosition[0], finalPosition[1] -1.953125\nweightedvalue -0.55078125\ncopyValueMap[state[0], state[1]] 0.0\naction [1, 0]\ninitial [1, 0]\naction [1, 0]\nreward -0.25\nvalueMap[finalPosition[0], finalPosition[1] 0.0\nweightedvalue -0.61328125\ncopyValueMap[state[0], state[1]] -1.55078125\naction [0, 1]\ninitial [1, 0]\naction [0, 1]\nreward -0.25\nvalueMap[finalPosition[0], finalPosition[1] 0.0\nweightedvalue -0.67578125\ncopyValueMap[state[0], state[1]] -1.61328125\naction [0, -1]\ninitial [1, 0]\naction [0, -1]\nreward -0.25\nvalueMap[finalPosition[0], finalPosition[1] -1.67578125\nweightedvalue -1.1572265625\ncopyValueMap[state[0], state[1]] -1.67578125\n--------------new State--------\n [1, 1]\naction [-1, 0]\ninitial [1, 1]\naction [-1, 0]\nreward -0.25\nvalueMap[finalPosition[0], finalPosition[1] -1.73828125\nweightedvalue -0.4970703125\ncopyValueMap[state[0], state[1]] 0.0\naction [1, 0]\ninitial [1, 1]\naction [1, 0]\nreward -0.25\nvalueMap[finalPosition[0], finalPosition[1] 0.0\nweightedvalue -0.5595703125\ncopyValueMap[state[0], state[1]] -1.4970703125\naction [0, 1]\ninitial [1, 1]\naction [0, 1]\nreward -0.25\nvalueMap[finalPosition[0], finalPosition[1] 0.0\nweightedvalue -0.6220703125\ncopyValueMap[state[0], state[1]] -1.5595703125\naction [0, -1]\ninitial [1, 1]\naction [0, -1]\nreward -0.25\nvalueMap[finalPosition[0], finalPosition[1] -2.1572265625\nweightedvalue -1.223876953125\ncopyValueMap[state[0], state[1]] -1.6220703125\n--------------new State--------\n [1, 2]\naction [-1, 0]\ninitial [1, 2]\naction [-1, 0]\nreward -0.25\nvalueMap[finalPosition[0], finalPosition[1] -1.6845703125\nweightedvalue -0.483642578125\ncopyValueMap[state[0], state[1]] 0.0\naction [1, 0]\ninitial [1, 2]\naction [1, 0]\nreward -0.25\nvalueMap[finalPosition[0], finalPosition[1] 0.0\nweightedvalue -0.546142578125\ncopyValueMap[state[0], state[1]] -1.483642578125\naction [0, 1]\ninitial [1, 2]\naction [0, 1]\nreward -0.25\nvalueMap[finalPosition[0], finalPosition[1] 0.0\nweightedvalue -0.608642578125\ncopyValueMap[state[0], state[1]] -1.546142578125\naction [0, -1]\ninitial [1, 2]\naction [0, -1]\nreward -0.25\nvalueMap[finalPosition[0], finalPosition[1] -2.223876953125\nweightedvalue -1.22711181640625\ncopyValueMap[state[0], state[1]] -1.608642578125\n--------------new State--------\n [1, 3]\naction [-1, 0]\ninitial [1, 3]\naction [-1, 0]\nreward -0.25\nvalueMap[finalPosition[0], finalPosition[1] -1.671142578125\nweightedvalue -0.48028564453125\ncopyValueMap[state[0], state[1]] 0.0\naction [1, 0]\ninitial [1, 3]\naction [1, 0]\nreward -0.25\nvalueMap[finalPosition[0], finalPosition[1] 0.0\nweightedvalue -0.54278564453125\ncopyValueMap[state[0], state[1]] -1.48028564453125\naction [0, 1]\ninitial [1, 3]\naction [0, 1]\nreward -0.25\nvalueMap[finalPosition[0], finalPosition[1] 0.0\nweightedvalue -0.60528564453125\ncopyValueMap[state[0], state[1]] -1.54278564453125\naction [0, -1]\ninitial [1, 3]\naction [0, -1]\nreward -0.25\nvalueMap[finalPosition[0], finalPosition[1] -2.22711181640625\nweightedvalue -1.2245635986328125\ncopyValueMap[state[0], state[1]] -1.60528564453125\n--------------new State--------\n [1, 4]\naction [-1, 0]\ninitial [1, 4]\naction [-1, 0]\nreward -0.25\nvalueMap[finalPosition[0], finalPosition[1] -1.94903564453125\nweightedvalue -0.5497589111328125\ncopyValueMap[state[0], state[1]] 0.0\naction [1, 0]\ninitial [1, 4]\naction [1, 0]\nreward -0.25\nvalueMap[finalPosition[0], finalPosition[1] 0.0\nweightedvalue -0.6122589111328125\ncopyValueMap[state[0], state[1]] -1.5497589111328125\naction [0, 1]\ninitial [1, 4]\naction [0, 1]\nreward -0.25\nvalueMap[finalPosition[0], finalPosition[1] -1.6122589111328125\nweightedvalue -1.0778236389160156\ncopyValueMap[state[0], state[1]] -1.6122589111328125\naction [0, -1]\ninitial [1, 4]\naction [0, -1]\nreward -0.25\nvalueMap[finalPosition[0], finalPosition[1] -2.2245635986328125\nweightedvalue -1.6964645385742188\ncopyValueMap[state[0], state[1]] -2.0778236389160156\n--------------new State--------\n [2, 0]\naction [-1, 0]\ninitial [2, 0]\naction [-1, 0]\nreward -0.25\nvalueMap[finalPosition[0], finalPosition[1] -2.1572265625\nweightedvalue -0.601806640625\ncopyValueMap[state[0], state[1]] 0.0\naction [1, 0]\ninitial [2, 0]\naction [1, 0]\nreward -0.25\nvalueMap[finalPosition[0], finalPosition[1] 0.0\nweightedvalue -0.664306640625\ncopyValueMap[state[0], state[1]] -1.601806640625\naction [0, 1]\ninitial [2, 0]\naction [0, 1]\nreward -0.25\nvalueMap[finalPosition[0], finalPosition[1] 0.0\nweightedvalue -0.726806640625\ncopyValueMap[state[0], state[1]] -1.664306640625\naction [0, -1]\ninitial [2, 0]\naction [0, -1]\nreward -0.25\nvalueMap[finalPosition[0], finalPosition[1] -1.726806640625\nweightedvalue -1.22100830078125\ncopyValueMap[state[0], state[1]] -1.726806640625\n--------------new State--------\n [2, 1]\naction [-1, 0]\ninitial [2, 1]\naction [-1, 0]\nreward -0.25\nvalueMap[finalPosition[0], finalPosition[1] -2.223876953125\nweightedvalue -0.61846923828125\ncopyValueMap[state[0], state[1]] 0.0\naction [1, 0]\ninitial [2, 1]\naction [1, 0]\nreward -0.25\nvalueMap[finalPosition[0], finalPosition[1] 0.0\nweightedvalue -0.68096923828125\ncopyValueMap[state[0], state[1]] -1.61846923828125\naction [0, 1]\ninitial [2, 1]\naction [0, 1]\nreward -0.25\nvalueMap[finalPosition[0], finalPosition[1] 0.0\nweightedvalue -0.74346923828125\ncopyValueMap[state[0], state[1]] -1.68096923828125\naction [0, -1]\ninitial [2, 1]\naction [0, -1]\nreward -0.25\nvalueMap[finalPosition[0], finalPosition[1] -2.22100830078125\nweightedvalue -1.3612213134765625\ncopyValueMap[state[0], state[1]] -1.74346923828125\n--------------new State--------\n [2, 2]\naction [-1, 0]\ninitial [2, 2]\naction [-1, 0]\nreward -0.25\nvalueMap[finalPosition[0], finalPosition[1] -2.22711181640625\nweightedvalue -0.6192779541015625\ncopyValueMap[state[0], state[1]] 0.0\naction [1, 0]\ninitial [2, 2]\naction [1, 0]\nreward -0.25\nvalueMap[finalPosition[0], finalPosition[1] 0.0\nweightedvalue -0.6817779541015625\ncopyValueMap[state[0], state[1]] -1.6192779541015625\naction [0, 1]\ninitial [2, 2]\naction [0, 1]\nreward -0.25\nvalueMap[finalPosition[0], finalPosition[1] 0.0\nweightedvalue -0.7442779541015625\ncopyValueMap[state[0], state[1]] -1.6817779541015625\naction [0, -1]\ninitial [2, 2]\naction [0, -1]\nreward -0.25\nvalueMap[finalPosition[0], finalPosition[1] -2.3612213134765625\nweightedvalue -1.3970832824707031\ncopyValueMap[state[0], state[1]] -1.7442779541015625\n--------------new State--------\n [2, 3]\naction [-1, 0]\ninitial [2, 3]\naction [-1, 0]\nreward -0.25\nvalueMap[finalPosition[0], finalPosition[1] -2.2245635986328125\nweightedvalue -0.6186408996582031\ncopyValueMap[state[0], state[1]] 0.0\naction [1, 0]\ninitial [2, 3]\naction [1, 0]\nreward -0.25\nvalueMap[finalPosition[0], finalPosition[1] 0.0\nweightedvalue -0.6811408996582031\ncopyValueMap[state[0], state[1]] -1.6186408996582031\naction [0, 1]\ninitial [2, 3]\naction [0, 1]\nreward -0.25\nvalueMap[finalPosition[0], finalPosition[1] 0.0\nweightedvalue -0.7436408996582031\ncopyValueMap[state[0], state[1]] -1.6811408996582031\naction [0, -1]\ninitial [2, 3]\naction [0, -1]\nreward -0.25\nvalueMap[finalPosition[0], finalPosition[1] -2.397083282470703\nweightedvalue -1.405411720275879\ncopyValueMap[state[0], state[1]] -1.7436408996582031\n--------------new State--------\n [2, 4]\naction [-1, 0]\ninitial [2, 4]\naction [-1, 0]\nreward -0.25\nvalueMap[finalPosition[0], finalPosition[1] -2.6964645385742188\nweightedvalue -0.7366161346435547\ncopyValueMap[state[0], state[1]] 0.0\naction [1, 0]\ninitial [2, 4]\naction [1, 0]\nreward -0.25\nvalueMap[finalPosition[0], finalPosition[1] 0.0\nweightedvalue -0.7991161346435547\ncopyValueMap[state[0], state[1]] -1.7366161346435547\naction [0, 1]\ninitial [2, 4]\naction [0, 1]\nreward -0.25\nvalueMap[finalPosition[0], finalPosition[1] -1.7991161346435547\nweightedvalue -1.3113951683044434\ncopyValueMap[state[0], state[1]] -1.7991161346435547\naction [0, -1]\ninitial [2, 4]\naction [0, -1]\nreward -0.25\nvalueMap[finalPosition[0], finalPosition[1] -2.405411720275879\nweightedvalue -1.975248098373413\ncopyValueMap[state[0], state[1]] -2.3113951683044434\n--------------new State--------\n [3, 0]\naction [-1, 0]\ninitial [3, 0]\naction [-1, 0]\nreward -0.25\nvalueMap[finalPosition[0], finalPosition[1] -2.22100830078125\nweightedvalue -0.6177520751953125\ncopyValueMap[state[0], state[1]] 0.0\naction [1, 0]\ninitial [3, 0]\naction [1, 0]\nreward -0.25\nvalueMap[finalPosition[0], finalPosition[1] 0.0\nweightedvalue -0.6802520751953125\ncopyValueMap[state[0], state[1]] -1.6177520751953125\naction [0, 1]\ninitial [3, 0]\naction [0, 1]\nreward -0.25\nvalueMap[finalPosition[0], finalPosition[1] 0.0\nweightedvalue -0.7427520751953125\ncopyValueMap[state[0], state[1]] -1.6802520751953125\naction [0, -1]\ninitial [3, 0]\naction [0, -1]\nreward -0.25\nvalueMap[finalPosition[0], finalPosition[1] -1.7427520751953125\nweightedvalue -1.2409400939941406\ncopyValueMap[state[0], state[1]] -1.7427520751953125\n--------------new State--------\n [3, 1]\naction [-1, 0]\ninitial [3, 1]\naction [-1, 0]\nreward -0.25\nvalueMap[finalPosition[0], finalPosition[1] -2.3612213134765625\nweightedvalue -0.6528053283691406\ncopyValueMap[state[0], state[1]] 0.0\naction [1, 0]\ninitial [3, 1]\naction [1, 0]\nreward -0.25\nvalueMap[finalPosition[0], finalPosition[1] 0.0\nweightedvalue -0.7153053283691406\ncopyValueMap[state[0], state[1]] -1.6528053283691406\naction [0, 1]\ninitial [3, 1]\naction [0, 1]\nreward -0.25\nvalueMap[finalPosition[0], finalPosition[1] 0.0\nweightedvalue -0.7778053283691406\ncopyValueMap[state[0], state[1]] -1.7153053283691406\naction [0, -1]\ninitial [3, 1]\naction [0, -1]\nreward -0.25\nvalueMap[finalPosition[0], finalPosition[1] -2.2409400939941406\nweightedvalue -1.4005403518676758\ncopyValueMap[state[0], state[1]] -1.7778053283691406\n--------------new State--------\n [3, 2]\naction [-1, 0]\ninitial [3, 2]\naction [-1, 0]\nreward -0.25\nvalueMap[finalPosition[0], finalPosition[1] -2.397083282470703\nweightedvalue -0.6617708206176758\ncopyValueMap[state[0], state[1]] 0.0\naction [1, 0]\ninitial [3, 2]\naction [1, 0]\nreward -0.25\nvalueMap[finalPosition[0], finalPosition[1] 0.0\nweightedvalue -0.7242708206176758\ncopyValueMap[state[0], state[1]] -1.6617708206176758\naction [0, 1]\ninitial [3, 2]\naction [0, 1]\nreward -0.25\nvalueMap[finalPosition[0], finalPosition[1] 0.0\nweightedvalue -0.7867708206176758\ncopyValueMap[state[0], state[1]] -1.7242708206176758\naction [0, -1]\ninitial [3, 2]\naction [0, -1]\nreward -0.25\nvalueMap[finalPosition[0], finalPosition[1] -2.400540351867676\nweightedvalue -1.4494059085845947\ncopyValueMap[state[0], state[1]] -1.7867708206176758\n--------------new State--------\n [3, 3]\naction [-1, 0]\ninitial [3, 3]\naction [-1, 0]\nreward -0.25\nvalueMap[finalPosition[0], finalPosition[1] -2.405411720275879\nweightedvalue -0.6638529300689697\ncopyValueMap[state[0], state[1]] 0.0\naction [1, 0]\ninitial [3, 3]\naction [1, 0]\nreward -0.25\nvalueMap[finalPosition[0], finalPosition[1] 0.0\nweightedvalue -0.7263529300689697\ncopyValueMap[state[0], state[1]] -1.6638529300689697\naction [0, 1]\ninitial [3, 3]\naction [0, 1]\nreward -0.25\nvalueMap[finalPosition[0], finalPosition[1] 0.0\nweightedvalue -0.7888529300689697\ncopyValueMap[state[0], state[1]] -1.7263529300689697\naction [0, -1]\ninitial [3, 3]\naction [0, -1]\nreward -0.25\nvalueMap[finalPosition[0], finalPosition[1] -2.4494059085845947\nweightedvalue -1.4637044072151184\ncopyValueMap[state[0], state[1]] -1.7888529300689697\n--------------new State--------\n [3, 4]\naction [-1, 0]\ninitial [3, 4]\naction [-1, 0]\nreward -0.25\nvalueMap[finalPosition[0], finalPosition[1] -2.975248098373413\nweightedvalue -0.8063120245933533\ncopyValueMap[state[0], state[1]] 0.0\naction [1, 0]\ninitial [3, 4]\naction [1, 0]\nreward -0.25\nvalueMap[finalPosition[0], finalPosition[1] 0.0\nweightedvalue -0.8688120245933533\ncopyValueMap[state[0], state[1]] -1.8063120245933533\naction [0, 1]\ninitial [3, 4]\naction [0, 1]\nreward -0.25\nvalueMap[finalPosition[0], finalPosition[1] -1.8688120245933533\nweightedvalue -1.3985150307416916\ncopyValueMap[state[0], state[1]] -1.8688120245933533\naction [0, -1]\ninitial [3, 4]\naction [0, -1]\nreward -0.25\nvalueMap[finalPosition[0], finalPosition[1] -2.4637044072151184\nweightedvalue -2.076941132545471\ncopyValueMap[state[0], state[1]] -2.3985150307416916\n--------------new State--------\n [4, 0]\naction [-1, 0]\ninitial [4, 0]\naction [-1, 0]\nreward -0.25\nvalueMap[finalPosition[0], finalPosition[1] -2.2409400939941406\nweightedvalue -0.6227350234985352\ncopyValueMap[state[0], state[1]] 0.0\naction [1, 0]\ninitial [4, 0]\naction [1, 0]\nreward -0.25\nvalueMap[finalPosition[0], finalPosition[1] -1.6227350234985352\nweightedvalue -1.090918779373169\ncopyValueMap[state[0], state[1]] -1.6227350234985352\naction [0, 1]\ninitial [4, 0]\naction [0, 1]\nreward -0.25\nvalueMap[finalPosition[0], finalPosition[1] 0.0\nweightedvalue -1.153418779373169\ncopyValueMap[state[0], state[1]] -2.090918779373169\naction [0, -1]\ninitial [4, 0]\naction [0, -1]\nreward -0.25\nvalueMap[finalPosition[0], finalPosition[1] -2.153418779373169\nweightedvalue -1.7542734742164612\ncopyValueMap[state[0], state[1]] -2.153418779373169\n--------------new State--------\n [4, 1]\naction [-1, 0]\ninitial [4, 1]\naction [-1, 0]\nreward -0.25\nvalueMap[finalPosition[0], finalPosition[1] -2.400540351867676\nweightedvalue -0.662635087966919\ncopyValueMap[state[0], state[1]] 0.0\naction [1, 0]\ninitial [4, 1]\naction [1, 0]\nreward -0.25\nvalueMap[finalPosition[0], finalPosition[1] -1.662635087966919\nweightedvalue -1.1407938599586487\ncopyValueMap[state[0], state[1]] -1.662635087966919\naction [0, 1]\ninitial [4, 1]\naction [0, 1]\nreward -0.25\nvalueMap[finalPosition[0], finalPosition[1] 0.0\nweightedvalue -1.2032938599586487\ncopyValueMap[state[0], state[1]] -2.1407938599586487\naction [0, -1]\ninitial [4, 1]\naction [0, -1]\nreward -0.25\nvalueMap[finalPosition[0], finalPosition[1] -2.754273474216461\nweightedvalue -1.954362228512764\ncopyValueMap[state[0], state[1]] -2.2032938599586487\n--------------new State--------\n [4, 2]\naction [-1, 0]\ninitial [4, 2]\naction [-1, 0]\nreward -0.25\nvalueMap[finalPosition[0], finalPosition[1] -2.4494059085845947\nweightedvalue -0.6748514771461487\ncopyValueMap[state[0], state[1]] 0.0\naction [1, 0]\ninitial [4, 2]\naction [1, 0]\nreward -0.25\nvalueMap[finalPosition[0], finalPosition[1] -1.6748514771461487\nweightedvalue -1.1560643464326859\ncopyValueMap[state[0], state[1]] -1.6748514771461487\naction [0, 1]\ninitial [4, 2]\naction [0, 1]\nreward -0.25\nvalueMap[finalPosition[0], finalPosition[1] 0.0\nweightedvalue -1.2185643464326859\ncopyValueMap[state[0], state[1]] -2.156064346432686\naction [0, -1]\ninitial [4, 2]\naction [0, -1]\nreward -0.25\nvalueMap[finalPosition[0], finalPosition[1] -2.954362228512764\nweightedvalue -2.019654903560877\ncopyValueMap[state[0], state[1]] -2.218564346432686\n--------------new State--------\n [4, 3]\naction [-1, 0]\ninitial [4, 3]\naction [-1, 0]\nreward -0.25\nvalueMap[finalPosition[0], finalPosition[1] -2.4637044072151184\nweightedvalue -0.6784261018037796\ncopyValueMap[state[0], state[1]] 0.0\naction [1, 0]\ninitial [4, 3]\naction [1, 0]\nreward -0.25\nvalueMap[finalPosition[0], finalPosition[1] -1.6784261018037796\nweightedvalue -1.1605326272547245\ncopyValueMap[state[0], state[1]] -1.6784261018037796\naction [0, 1]\ninitial [4, 3]\naction [0, 1]\nreward -0.25\nvalueMap[finalPosition[0], finalPosition[1] 0.0\nweightedvalue -1.2230326272547245\ncopyValueMap[state[0], state[1]] -2.1605326272547245\naction [0, -1]\ninitial [4, 3]\naction [0, -1]\nreward -0.25\nvalueMap[finalPosition[0], finalPosition[1] -3.019654903560877\nweightedvalue -2.0404463531449437\ncopyValueMap[state[0], state[1]] -2.2230326272547245\n--------------new State--------\n [4, 4]\naction [-1, 0]\nreward 0\nvalueMap[finalPosition[0], finalPosition[1] 0.0\nweightedvalue 0.0\ncopyValueMap[state[0], state[1]] 0.0\naction [1, 0]\nreward 0\nvalueMap[finalPosition[0], finalPosition[1] -1.0\nweightedvalue -0.25\ncopyValueMap[state[0], state[1]] -1.0\naction [0, 1]\nreward 0\nvalueMap[finalPosition[0], finalPosition[1] -1.25\nweightedvalue -0.5625\ncopyValueMap[state[0], state[1]] -1.25\naction [0, -1]\nreward 0\nvalueMap[finalPosition[0], finalPosition[1] -1.5625\nweightedvalue -0.953125\ncopyValueMap[state[0], state[1]] -1.5625\nIteration 0\n[[-1.953125   -1.73828125 -1.68457031 -1.67114258 -1.94903564]\n [-2.15722656 -2.22387695 -2.22711182 -2.2245636  -2.69646454]\n [-2.2210083  -2.36122131 -2.39708328 -2.40541172 -2.9752481 ]\n [-2.24094009 -2.40054035 -2.44940591 -2.46370441 -3.07694113]\n [-2.75427347 -2.95436223 -3.0196549  -3.04044635 -1.953125  ]]\n\n"
     ]
    }
   ],
   "source": [
    "gamma = 1 # discounting rate\n",
    "rewardSize = -0.25\n",
    "gridSize = 5\n",
    "terminationStates = [[0,0], [gridSize-1, gridSize-1]]\n",
    "actions = [[-1, 0], [1, 0], [0, 1], [0, -1]]\n",
    "numIterations = 1\n",
    "def actionRewardFunction(initialPosition, action):\n",
    "    if initialPosition in terminationStates:\n",
    "        return initialPosition, 0\n",
    "    reward = rewardSize\n",
    "    #print(\"reward\",reward)\n",
    "    print(\"initial\",initialPosition)\n",
    "    print(\"action\",action)\n",
    "    finalPosition = np.array(initialPosition) + np.array(action)\n",
    "    #print(\"finalPosition\",finalPosition)\n",
    "    if -1 in finalPosition or 5 in finalPosition: \n",
    "        finalPosition = initialPosition\n",
    "        #print(\"finalPosition\",finalPosition)\n",
    "    return finalPosition, reward\n",
    "\n",
    "valueMap = np.zeros((gridSize, gridSize))\n",
    "print(\"valuemap\",range(gridSize))\n",
    "states = [[i, j] for i in range(gridSize) for j in range(gridSize)]\n",
    "print(\"'States\",states)\n",
    "valueMap\n",
    "deltas = []\n",
    "for it in range(numIterations):\n",
    "    copyValueMap = np.copy(valueMap)\n",
    "    deltaState = []\n",
    "    for state in states:\n",
    "        print(\"--------------new State--------\\n\",state)\n",
    "        weightedRewards = 0\n",
    "        actions = [[-1, 0], [1, 0], [0, 1], [0, -1]]\n",
    "        for action in actions:\n",
    "            print(\"action\",action)\n",
    "            #print(\"length\",(1/len(actions)))\n",
    "            finalPosition, reward = actionRewardFunction(state, action)\n",
    "            print(\"reward\",reward)\n",
    "            #print(\"finalPosition[0]\",finalPosition[0])\n",
    "            #print(\"finalPosition[1]\",finalPosition[1])\n",
    "            print(\"valueMap[finalPosition[0], finalPosition[1]\",valueMap[finalPosition[0], finalPosition[1]])\n",
    "\n",
    "            weightedRewards += ((1/len(actions))*(reward+(gamma*valueMap[finalPosition[0], finalPosition[1]])))\n",
    "            print(\"weightedvalue\",weightedRewards)\n",
    "            print(\"copyValueMap[state[0], state[1]]\",copyValueMap[state[0], state[1]])\n",
    "            #deltaState.append(np.abs(copyValueMap[state[0], state[1]]-weightedRewards))\n",
    "            #deltaState.append(np.abs(-1 + weightedRewards))\n",
    "           #print(\"deltaState\",deltaState)\n",
    "            copyValueMap[state[0], state[1]] = -1+ weightedRewards\n",
    "            #deltas.append(deltaState)\n",
    "            valueMap = copyValueMap\n",
    "    if it in [0,1,2,9, 99, numIterations-1]:\n",
    "        print(\"Iteration {}\".format(it))\n",
    "        print(valueMap)\n",
    "        print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}